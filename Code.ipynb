{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn.metrics as sk\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import svm\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train_E6oV3lV.csv', encoding='latin-1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('id',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,Y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23971,)\n",
      "(7991,)\n",
      "(23971,)\n",
      "(7991,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df = vect.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_df = vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_df,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=model.predict(X_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9609560755850332"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_test,p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import re, collections\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import cohen_kappa_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_wordlist(raw_sentence):\n",
    "    \n",
    "    clean_sentence = re.sub(\"[^a-zA-Z0-9]\",\" \", raw_sentence)\n",
    "    tokens = nltk.word_tokenize(clean_sentence)\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(essay):\n",
    "    stripped_essay = essay.strip()\n",
    "    \n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    raw_sentences = tokenizer.tokenize(stripped_essay)\n",
    "    \n",
    "    tokenized_sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            tokenized_sentences.append(sentence_to_wordlist(raw_sentence))\n",
    "    \n",
    "    return tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_word_len(essay):\n",
    "    \n",
    "    clean_essay = re.sub(r'\\W', ' ', essay)\n",
    "    words = nltk.word_tokenize(clean_essay)\n",
    "    \n",
    "    return sum(len(word) for word in words) / len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count(essay):\n",
    "    \n",
    "    clean_essay = re.sub(r'\\W', ' ', essay)\n",
    "    words = nltk.word_tokenize(clean_essay)\n",
    "    \n",
    "    return len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_count(essay):\n",
    "    \n",
    "    clean_essay = re.sub(r'\\s', '', str(essay).lower())\n",
    "    \n",
    "    return len(clean_essay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_count(essay):\n",
    "    \n",
    "    sentences = nltk.sent_tokenize(essay)\n",
    "    \n",
    "    return len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_lemmas(essay):\n",
    "    \n",
    "    tokenized_sentences = tokenize(essay)      \n",
    "    \n",
    "    lemmas = []\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    for sentence in tokenized_sentences:\n",
    "        tagged_tokens = nltk.pos_tag(sentence) \n",
    "        \n",
    "        for token_tuple in tagged_tokens:\n",
    "        \n",
    "            pos_tag = token_tuple[1]\n",
    "        \n",
    "            if pos_tag.startswith('N'): \n",
    "                pos = wordnet.NOUN\n",
    "                lemmas.append(wordnet_lemmatizer.lemmatize(token_tuple[0], pos))\n",
    "            elif pos_tag.startswith('J'):\n",
    "                pos = wordnet.ADJ\n",
    "                lemmas.append(wordnet_lemmatizer.lemmatize(token_tuple[0], pos))\n",
    "            elif pos_tag.startswith('V'):\n",
    "                pos = wordnet.VERB\n",
    "                lemmas.append(wordnet_lemmatizer.lemmatize(token_tuple[0], pos))\n",
    "            elif pos_tag.startswith('R'):\n",
    "                pos = wordnet.ADV\n",
    "                lemmas.append(wordnet_lemmatizer.lemmatize(token_tuple[0], pos))\n",
    "            else:\n",
    "                pos = wordnet.NOUN\n",
    "                lemmas.append(wordnet_lemmatizer.lemmatize(token_tuple[0], pos))\n",
    "    \n",
    "    lemma_count = len(set(lemmas))\n",
    "    \n",
    "    return lemma_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_pos(essay):\n",
    "    \n",
    "    tokenized_sentences = tokenize(essay)\n",
    "    \n",
    "    noun_count = 0\n",
    "    adj_count = 0\n",
    "    verb_count = 0\n",
    "    adv_count = 0\n",
    "    \n",
    "    for sentence in tokenized_sentences:\n",
    "        tagged_tokens = nltk.pos_tag(sentence)\n",
    "        \n",
    "        for token_tuple in tagged_tokens:\n",
    "            pos_tag = token_tuple[1]\n",
    "        \n",
    "            if pos_tag.startswith('N'): \n",
    "                noun_count += 1\n",
    "            elif pos_tag.startswith('J'):\n",
    "                adj_count += 1\n",
    "            elif pos_tag.startswith('V'):\n",
    "                verb_count += 1\n",
    "            elif pos_tag.startswith('R'):\n",
    "                adv_count += 1\n",
    "            \n",
    "    return noun_count, adj_count, verb_count, adv_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(data):\n",
    "    \n",
    "    features = data.copy()\n",
    "    \n",
    "    features['char_count'] = features['tweet'].apply(char_count)\n",
    "    \n",
    "    features['word_count'] = features['tweet'].apply(word_count)\n",
    "    \n",
    "    features['sent_count'] = features['tweet'].apply(sent_count)\n",
    "    \n",
    "    features['avg_word_len'] = features['tweet'].apply(avg_word_len)\n",
    "    \n",
    "    features['lemma_count'] = features['tweet'].apply(count_lemmas)\n",
    "    \n",
    "    \n",
    "    features['noun_count'], features['adj_count'], features['verb_count'], features['adv_count'] = zip(*features['tweet'].map(count_pos))\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\MyHomePC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\MyHomePC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\MyHomePC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       label                                              tweet  char_count  \\\n",
      "0          0   @user when a father is dysfunctional and is s...          82   \n",
      "1          0  @user @user thanks for #lyft credit i can't us...         101   \n",
      "2          0                                bihday your majesty          17   \n",
      "3          0  #model   i love u take with u all the time in ...         101   \n",
      "4          0             factsguide: society now    #motivation          32   \n",
      "5          0  [2/2] huge fan fare and big talking before the...          96   \n",
      "6          0   @user camping tomorrow @user @user @user @use...          66   \n",
      "7          0  the next school year is the year for exams.Ã°Â...         129   \n",
      "8          0  we won!!! love the land!!! #allin #cavs #champ...          78   \n",
      "9          0   @user @user welcome here !  i'm   it's so #gr...          36   \n",
      "10         0   Ã¢ÂÂ #ireland consumer price index (mom) cl...          94   \n",
      "11         0  we are so selfish. #orlando #standwithorlando ...         118   \n",
      "12         0  i get to see my daddy today!!   #80days #getti...          41   \n",
      "13         1  @user #cnn calls #michigan middle school 'buil...          61   \n",
      "14         1  no comment!  in #australia   #opkillingbay #se...          89   \n",
      "15         0  ouch...junior is angryÃ°ÂÂÂ#got7 #junior #y...          52   \n",
      "16         0  i am thankful for having a paner. #thankful #p...          45   \n",
      "17         1                             retweet if you agree!           18   \n",
      "18         0  its #friday! Ã°ÂÂÂ smiles all around via ig...          67   \n",
      "19         0  as we all know, essential oils are not made of...          47   \n",
      "20         0  #euro2016 people blaming ha for conceded goal ...         103   \n",
      "21         0  sad little dude..   #badday #coneofshame #cats...          59   \n",
      "22         0  product of the day: happy man #wine tool  who'...          79   \n",
      "23         1    @user @user lumpy says i am a . prove it lumpy.          37   \n",
      "24         0   @user #tgif   #ff to my #gamedev #indiedev #i...          79   \n",
      "25         0  beautiful sign by vendor 80 for $45.00!! #upsi...          67   \n",
      "26         0   @user all #smiles when #media is   !! Ã°ÂÂÂ...         128   \n",
      "27         0  we had a great panel on the mediatization of t...          58   \n",
      "28         0  happy father's day @user Ã°ÂÂÂÃ°ÂÂÂÃ°ÂÂ...          53   \n",
      "29         0  50 people went to nightclub to have a good nig...         112   \n",
      "...      ...                                                ...         ...   \n",
      "31932      0                               @user thanks gemma            16   \n",
      "31933      1  @user judd is a  &amp; #homophobic #freemilo #...          93   \n",
      "31934      1  lady banned from kentucky mall. @user  #jcpenn...          49   \n",
      "31935      0  ugh i'm trying to enjoy my happy hour drink &a...         110   \n",
      "31936      0  want to know how to live a   life? do more thi...         100   \n",
      "31937      0                             love island Ã°ÂÂÂ            18   \n",
      "31938      0  my fav actor #vijaysethupathi ! my fav actress...          89   \n",
      "31939      0  whew  Ã°ÂÂÂ",
      " it's a productive and   #friday!!!          39   \n",
      "31940      0                 @user she's finally here! @user             27   \n",
      "31941      0  passed first year of uni #yay #love #pass #uni...          76   \n",
      "31942      0  this week is flying by   #humpday - #wednesday...          53   \n",
      "31943      0   @user modeling photoshoot this friday yay #mo...          56   \n",
      "31944      0  you're surrounded by people who love you (even...          81   \n",
      "31945      0  feel like... Ã°ÂÂÂÃ°ÂÂÂ¶Ã°ÂÂÂ #dog #su...          68   \n",
      "31946      1  @user omfg i'm offended! i'm a  mailbox and i'...          69   \n",
      "31947      1  @user @user you don't have the balls to hashta...          89   \n",
      "31948      1   makes you ask yourself, who am i? then am i a...          69   \n",
      "31949      0  hear one of my new songs! don't go - katie ell...          90   \n",
      "31950      0   @user you can try to 'tail' us to stop, 'butt...          91   \n",
      "31951      0  i've just posted a new blog: #secondlife #lone...          46   \n",
      "31952      0                @user you went too far with @user            27   \n",
      "31953      0  good morning #instagram #shower #water #berlin...          89   \n",
      "31954      0  #holiday   bull up: you will dominate your bul...          85   \n",
      "31955      0  less than 2 weeks Ã°ÂÂÂ",
      "Ã°ÂÂÂÃ°ÂÂÂ¼Ã°Â...         107   \n",
      "31956      0  off fishing tomorrow @user carnt wait first ti...          49   \n",
      "31957      0  ate @user isz that youuu?Ã°ÂÂÂÃ°ÂÂÂÃ°ÂÂ...         105   \n",
      "31958      0    to see nina turner on the airwaves trying to...         107   \n",
      "31959      0  listening to sad songs on a monday morning otw...          49   \n",
      "31960      1  @user #sikh #temple vandalised in in #calgary,...          55   \n",
      "31961      0                   thank you @user for you follow            25   \n",
      "\n",
      "       word_count  sent_count  avg_word_len  lemma_count  noun_count  \\\n",
      "0              18           2      4.388889           16           4   \n",
      "1              21           2      4.428571           19           9   \n",
      "2               3           1      5.666667            3           1   \n",
      "3              43           2      1.534884           11           4   \n",
      "4               4           1      7.500000            4           2   \n",
      "5              20           3      4.500000           17           5   \n",
      "6              13           1      4.230769            4           4   \n",
      "7              29           1      3.862069           18           8   \n",
      "8              13           3      4.923077           10           5   \n",
      "9              10           2      2.900000            9           2   \n",
      "10             22           1      3.636364           18           5   \n",
      "11             13           2      8.230769           12           3   \n",
      "12              9           2      4.111111            9           4   \n",
      "13             11           1      4.818182           11           5   \n",
      "14              9           2      9.111111            8           4   \n",
      "15             11           1      3.727273            7           4   \n",
      "16              9           2      4.666667            8           3   \n",
      "17              4           1      4.250000            4           1   \n",
      "18             16           2      3.625000           11           7   \n",
      "19             11           1      4.090909           11           2   \n",
      "20             23           1      4.391304           23           6   \n",
      "21              9           1      5.666667            9           5   \n",
      "22             21           2      3.333333           18           8   \n",
      "23             10           2      3.300000            8           3   \n",
      "24             14           2      4.714286            9          10   \n",
      "25             11           2      5.454545           11           4   \n",
      "26             35           4      2.714286           13           7   \n",
      "27             13           1      4.384615           12           4   \n",
      "28             21           1      1.666667            5           3   \n",
      "29             26           1      4.192308           23           7   \n",
      "...           ...         ...           ...          ...         ...   \n",
      "31932           3           1      5.000000            3           1   \n",
      "31933          15           1      5.333333            8          12   \n",
      "31934           8           2      5.625000            7           5   \n",
      "31935          28           2      3.750000           25           8   \n",
      "31936          26           3      3.692308           20           5   \n",
      "31937           6           1      2.333333            2           2   \n",
      "31938          18           5      4.277778           13           9   \n",
      "31939          11           2      2.818182            7           1   \n",
      "31940           6           2      3.833333            5           2   \n",
      "31941          12           1      5.750000           12           5   \n",
      "31942          11           1      4.090909            9           3   \n",
      "31943          10           1      5.100000            9           5   \n",
      "31944          19           1      4.000000           17           1   \n",
      "31945          21           1      2.190476            9           5   \n",
      "31946          15           3      4.066667           11           9   \n",
      "31947          24           1      3.375000           20           5   \n",
      "31948          17           4      3.529412           13           5   \n",
      "31949          17           2      4.764706           16           7   \n",
      "31950          21           2      3.857143           20           4   \n",
      "31951          10           1      4.100000           10           3   \n",
      "31952           7           1      3.571429            6           2   \n",
      "31953          13           1      6.000000           13          11   \n",
      "31954          21           2      3.904762           15           2   \n",
      "31955          32           1      2.531250           10           5   \n",
      "31956          11           1      4.363636           11           5   \n",
      "31957          47           1      1.297872            5           3   \n",
      "31958          23           2      4.521739           21           8   \n",
      "31959          13           1      3.769231           11           3   \n",
      "31960          10           1      4.900000            9           6   \n",
      "31961           6           1      4.000000            5           1   \n",
      "\n",
      "       adj_count  verb_count  adv_count  \n",
      "0              2           4          1  \n",
      "1              3           4          1  \n",
      "2              0           0          1  \n",
      "3              3           1          0  \n",
      "4              1           0          1  \n",
      "5              2           4          1  \n",
      "6              0           1          0  \n",
      "7              2           4          1  \n",
      "8              0           3          0  \n",
      "9              2           2          3  \n",
      "10             1           5          0  \n",
      "11             3           5          1  \n",
      "12             0           2          0  \n",
      "13             3           2          0  \n",
      "14             2           1          0  \n",
      "15             3           1          0  \n",
      "16             2           2          0  \n",
      "17             0           1          0  \n",
      "18             0           1          0  \n",
      "19             1           3          1  \n",
      "20             4           5          1  \n",
      "21             3           1          0  \n",
      "22             1           4          2  \n",
      "23             1           3          1  \n",
      "24             1           0          1  \n",
      "25             2           0          0  \n",
      "26             1           1          1  \n",
      "27             2           1          0  \n",
      "28             1           1          0  \n",
      "29             2           7          1  \n",
      "...          ...         ...        ...  \n",
      "31932          1           1          0  \n",
      "31933          1           1          0  \n",
      "31934          1           1          0  \n",
      "31935          2           6          0  \n",
      "31936          3           6          1  \n",
      "31937          0           0          0  \n",
      "31938          3           1          0  \n",
      "31939          2           1          0  \n",
      "31940          0           1          2  \n",
      "31941          4           2          0  \n",
      "31942          2           2          0  \n",
      "31943          1           2          0  \n",
      "31944          2           5          3  \n",
      "31945          2           1          0  \n",
      "31946          2           2          0  \n",
      "31947          2           6          2  \n",
      "31948          0           5          1  \n",
      "31949          4           2          1  \n",
      "31950          1           5          2  \n",
      "31951          3           1          2  \n",
      "31952          0           1          2  \n",
      "31953          2           0          0  \n",
      "31954          0           5          0  \n",
      "31955          1           1          1  \n",
      "31956          1           1          0  \n",
      "31957          0           1          0  \n",
      "31958          2           3          0  \n",
      "31959          3           3          0  \n",
      "31960          1           1          0  \n",
      "31961          0           2          0  \n",
      "\n",
      "[31962 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "features_set1 = extract_features(df)\n",
    "\n",
    "print(features_set1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = features_set1.iloc[:, 3:].as_matrix()\n",
    "\n",
    "y = features_set1['label'].as_matrix()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1000,\n",
      "            oob_score=False, random_state=10, verbose=0, warm_start=False)\n",
      "[[8867   44]\n",
      " [ 576  102]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "93.53425800396288"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn.metrics as sk\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import accuracy_score\n",
    "clf = RandomForestClassifier(n_jobs=1000, random_state=10,n_estimators=10)\n",
    "print(clf.fit(X_train,y_train))\n",
    "p= clf.predict(X_test)\n",
    "print(sk.confusion_matrix(y_test,p))\n",
    "accuracy_score(y_test,p)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['lable'] = Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31962, 9)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=df1.take(np.random.permutation(len(df))[:3000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv(\"final.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 2691\n",
      "Test set: 309\n",
      "Accuracy: 89.96763754045307%\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import random\n",
    "import math\n",
    "import operator\n",
    " \n",
    "def loadDataset(filename, split, trainingSet=[] , testSet=[]):\n",
    "    with open(filename, 'r') as csvfile:\n",
    "        lines = csv.reader(csvfile)\n",
    "        dataset = list(lines)\n",
    "        dataset = dataset[1:]\n",
    "        for x in range(len(dataset)):\n",
    "            for y in range(9):         # 0 to 7 , the numeric values only\n",
    "                dataset[x][y] = float(dataset[x][y])\n",
    "            if random.random() < split:\n",
    "                trainingSet.append(dataset[x])\n",
    "            else:\n",
    "                testSet.append(dataset[x])\n",
    " \n",
    " \n",
    "def euclideanDistance(instance1, instance2, length):\n",
    "    distance = 0\n",
    "    for x in range(length):\n",
    "        distance += pow((instance1[x] - instance2[x]), 2)\n",
    "    return math.sqrt(distance)\n",
    " \n",
    "def getNeighbors(trainingSet, testInstance, k):\n",
    "    distances = []\n",
    "    length = len(testInstance)-1\n",
    "    for x in range(len(trainingSet)):\n",
    "        dist = euclideanDistance(testInstance, trainingSet[x], length)\n",
    "        distances.append((trainingSet[x], dist))\n",
    "    distances.sort(key=operator.itemgetter(1))     # Sort distances in ascending order\n",
    "    neighbors = []\n",
    "    for x in range(k):                             # k nearest neighbors\n",
    "        neighbors.append(distances[x][0])\n",
    "    return neighbors\n",
    " \n",
    "def getResponse(neighbors):\n",
    "    classVotes = {}\n",
    "    for x in range(len(neighbors)):\n",
    "        response = neighbors[x][-1]\n",
    "        if response in classVotes:\n",
    "            classVotes[response] += 1\n",
    "        else:\n",
    "            classVotes[response] = 1\n",
    "    sortedVotes = sorted(classVotes.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    return sortedVotes[0][0]\n",
    " \n",
    "def getAccuracy(testSet, predictions):\n",
    "    correct = 0\n",
    "    for x in range(len(testSet)):\n",
    "        if testSet[x][-1] == predictions[x]:\n",
    "            correct += 1\n",
    "    return (correct/float(len(testSet))) * 100.0\n",
    "\n",
    "trainingSet=[]\n",
    "testSet=[]\n",
    "split = 0.9\n",
    "loadDataset('final.csv', split, trainingSet, testSet)\n",
    "print('Train set: ' + repr(len(trainingSet)))\n",
    "print('Test set: ' + repr(len(testSet)))\n",
    "# generate predictions\n",
    "predictions=[]\n",
    "k = 3\n",
    "for x in range(len(testSet)):\n",
    "    neighbors = getNeighbors(trainingSet, testSet[x], k)\n",
    "    result = getResponse(neighbors)\n",
    "    predictions.append(result)\n",
    "accuracy = getAccuracy(testSet, predictions)\n",
    "print('Accuracy: ' + repr(accuracy) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
